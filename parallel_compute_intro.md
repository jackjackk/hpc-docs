# Introduction
This guide provides a practical overview of setting up and running parallel computing R and Python scripts on the [Institute for CyberScience Advanced CyberInfrastructure (ICS-ACI)](https://ics.psu.edu/advanced-cyberinfrastructure/). ICS-ACI is the high-performance computing (HPC) at Penn State. For an introduction and further background on ICS-ACI, please see the following resources:

* [ICS-ACI Open Queue onboarding video](https://youtu.be/RQSrK1BH1-w) and accompanying [PowerPoint](https://psu.box.com/s/7o1bhgzxp01av9le578ub7ifb5mo0waw)
* [Guide to ICS-ACI for SCRiM Users](http://www.scrimhub.org/support/aci-scrim-cyberinfrastructure-guide.html)
* [ICS-ACI Quick Reference Document](https://ics.psu.edu/wp-content/uploads/2015/07/ACI-Documentation_Reference-Sheet.pdf)
* [ICS Training Videos](https://ics.psu.edu/what-we-do/education-and-training/ics-on-demand-videos/)

# What is parallel computing?
Parallel computing is the “simultaneous use of multiple compute resources to solve a computational problem” ([Barney 2017](https://computing.llnl.gov/tutorials/parallel_comp/#Whatis)). A workload is split into separate tasks, the tasks are executed concurrently on different computing resources, and then the results of the tasks are brought back together. There are many different parallel programming paradigms (see [Barney 2017](https://computing.llnl.gov/tutorials/parallel_comp/#Whatis) for a full review), but for practical purposes, it’s useful to think of two different models: (1) a shared memory multithreaded model; and (2) a distributed memory multiprocess model.

## Shared memory multithreaded model
Under the shared memory multithreaded model, a parent process has offshoots of concurrent execution called threads that all have access to the same shared memory. The threads can simultaneously run across multiple different processor cores, thus enabling parallelization. Because the threads share the same data structures, it is easy for them to communicate. However, this means proper [thread-safe code](https://en.wikipedia.org/wiki/Thread_safety) must be implemented to avoid [race conditions](https://stackoverflow.com/questions/34510/what-is-a-race-condition) and data inconsistencies. For instance, if multiple threads can change the value of a variable at the same time, [unexpected results can occur](https://stackoverflow.com/questions/34510/what-is-a-race-condition). Typically, shared memory multithreaded implementations can only be executed on a single computer. They can be run on HPC systems like ICS-ACI, but can only execute on a single node (i.e. computer) and cannot be used to execute a workload across multiple different nodes. In scientific computing, the most popular implementation of shared memory multithreading is the [OpenMP application programming interface](http://www.openmp.org/) for C/C++ and Fortran. Multithreading libraries are also available in [Python](https://docs.python.org/3.6/library/threading.html) and [R](https://cran.r-project.org/web/packages/Rdsm/index.html). It is important to note that while the Python threading library can be useful in [specific cases](https://www.toptal.com/python/beginners-guide-to-concurrency-and-parallelism-in-python), it is limited by the [Global Interpreter Lock](https://en.wikipedia.org/wiki/Global_interpreter_lock).

## Distributed memory multiprocess model
In the distributed memory multiprocess model, each parallel line of execution is a true independent process with its own memory space. Because the processes do not share memory, they coordinate and exchange data via messages. Although the message passing can occur added latency and decreased performance relative to a multithreaded model, the overhead required to ensure thread-safe code is eliminated. Workload processes can exist on the same node or across nodes, but multiprocessing libraries often only support execution on a single node (e.g. [Python multiprocessing package](https://docs.python.org/3.6/library/multiprocessing.html)). In the next section, we will discuss the [Message Passing Interface (MPI)](https://computing.llnl.gov/tutorials/mpi), the industry standard for executing a distributed memory multiprocess model workload across nodes on a HPC system.   